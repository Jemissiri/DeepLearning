{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02972eec",
   "metadata": {},
   "source": [
    "# Lab 2: Backpropagation for NXOR\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc026d4f-4cfb-4f95-b255-17baacfe44e9",
   "metadata": {},
   "source": [
    "Grupo 3 \\\n",
    "Alexandre Rodrigues: 75545 \\\n",
    "Tiago Granja: 79845 \\\n",
    "Diogo Silva: 79828"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda23db",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38082d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45194f73-3ae3-474b-84b3-f04a621896c4",
   "metadata": {},
   "source": [
    "---\n",
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d2ed7-4e3c-4aae-b945-a5a110c39f93",
   "metadata": {},
   "source": [
    "To calculate the weights update we first need to feedforward our network. With our 2:1 architecture we have 6 weights, 4 in the hidden layer, $w_{11}^{[1]}, w_{21}^{[1]}, w_{12}^{[1]}, w_{22}^{[1]}$, and 2 in the output layer, $w_1^{[2]}, w_2^{[2]}$, and 3 biases, $b_1^{[1]}, b_2^{[1]}$, in the first layer and $b_1^{[2]}$ in the second. With this we can write our inner weighted sums and outputs as follows:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "z_1^{[1]} = x_1 w_{11}^{[1]} + x_2 w_{21}^{[1]} + b_1^{[1]} \\\\\n",
    "y_1^{[1]} = S(z_1^{[1]}) \\\\\n",
    "\\\\\n",
    "z_2^{[1]} = x_1 w_{12}^{[1]} + x_2 w_{22}^{[1]} + b_2^{[1]} \\\\\n",
    "y_2^{[1]} = S(z_2^{[1]}) \\\\\n",
    "\\\\\n",
    "z_1^{[2]} = y_1^{[1]} w_1^{[2]} + y_2^{[1]} w_2^{[2]} + b_1^{[2]} \\\\\n",
    "y = S(z_1^{[2]})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where S is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b493d-3d17-4227-a120-1ebda704d3fd",
   "metadata": {},
   "source": [
    "#### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba268ba6-53b7-48a7-ae4c-6651f90942ac",
   "metadata": {},
   "source": [
    "For backpropagation we use the squared error as our error where $E = \\frac{1}{2}(y - t)^2$, where $t$ is our target output. With this we can start writing the expressions for the weight updates such that:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "w_{ij}^{[k]} = w_{ij}^{[k]} - \\upeta \\frac{\\partial E}{\\partial w_{ij}^{[k]}}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d69f74-e028-4b23-8492-b14bebf1ec0e",
   "metadata": {},
   "source": [
    "##### Outer layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d934c-27d5-4789-a6b4-3f26f4c2a9b4",
   "metadata": {},
   "source": [
    "For the outer layer we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20806c67-3dac-4b54-a37e-e6722db0cd64",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\Delta w_{i}^{[2]} = \\frac{\\partial E}{\\partial w_{i}^{[2]}} \\\\\n",
    "\\Delta w_{i}^{[2]} = \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial z_1^{[2]}}\\frac{\\partial z_1^{[2]}}{\\partial w_i^{[2]}} \\\\\n",
    "\\Delta w_{i}^{[2]} = (y - t) S'(z_1^{[2]})y_i^{[1]} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where the last term is equal to one when taking the partial derivative in order to $b_1^{[2]}$ and $S'$ is the derivative of $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c6d2d-0c44-48b3-b2d4-ee64b61b09d1",
   "metadata": {},
   "source": [
    "##### Inner (Hidden) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0043363-39f7-4ca0-b1e7-df5adfdd88bb",
   "metadata": {},
   "source": [
    "For the inner layer we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e12933-1245-49c6-aa9b-eeb505d16f13",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\Delta w_{ij}^{[1]} = \\frac{\\partial E}{\\partial w_{ij}^{[1]}} \\\\\n",
    "\\Delta w_{ij}^{[1]} = \\frac{\\partial E}{\\partial y_j^{[1]}}\\frac{\\partial y_j^{[1]}}{\\partial z_j^{[1]}}\\frac{\\partial z_j^{[1]}}{\\partial w_{ij}^{[1]}} \\\\\n",
    "\\Delta w_{ij}^{[1]} = \\frac{\\partial E}{\\partial y_j^{[1]}} S'(z_j^{[1]}) x_i\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial E}{\\partial y_j^{[1]}} = \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial z_1^{[2]}}\\frac{\\partial z_1^{[2]}}{\\partial y_j^{[1]}} \\\\\n",
    "\\frac{\\partial E}{\\partial y_j^{[1]}} = (y - t) S'(z_1^{[2]}) w_j^{[2]}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Concluding:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\Delta w_{ij}^{[1]} = (y - t) S'(z_1^{[2]}) w_j^{[2]} S'(z_j^{[1]}) x_i\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where the last term is equal to one when taking the partial derivative in order to $b_1^2$ and $S'$ is the derivative of $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68698b-c638-43bc-ab4c-ab850231abe3",
   "metadata": {},
   "source": [
    "---\n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f406f735-ec2a-435c-94a9-7b2806ed3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x: float) -> float:\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d488834-ef5f-43df-af0a-4f40ffa542b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTwoOneNetwork:\n",
    "\n",
    "    def __init__(self, learning_rate: float):\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "        self._inner_weights = np.random.uniform(0.0, 0.1, (2, 2))\n",
    "        self._inner_biases = np.random.uniform(0.0, 0.1, 2)\n",
    "        self._outer_weights = np.random.uniform(0.0, 0.1, 2)\n",
    "        self._outer_bias = np.random.uniform(0.0, 0.1)\n",
    "\n",
    "        self._inner_sums = np.empty(2)\n",
    "        self._inner_outputs = np.empty(2)\n",
    "    \n",
    "    def _feed_forward(self, x1: float, x2:float) -> float:\n",
    "\n",
    "        # Inner layer\n",
    "        self._inner_sums[0] = self._inner_weights[0][0] * x1 + self._inner_weights[1][0] * x2 + self._inner_biases[0]\n",
    "        self._inner_outputs[0] = sigmoid(self._inner_sums[0])\n",
    "        self._inner_sums[1] = self._inner_weights[0][1] * x1 + self._inner_weights[1][1] * x2 + self._inner_biases[1]\n",
    "        self._inner_outputs[1] = sigmoid(self._inner_sums[1])\n",
    "\n",
    "        # Outer layer\n",
    "        self._outer_sum = self._outer_weights[0] * self._inner_outputs[0] + self._outer_weights[1] * self._inner_outputs[1] + self._outer_bias\n",
    "        self._output = sigmoid(self._outer_sum)\n",
    "\n",
    "        return self._output\n",
    "\n",
    "    def _backpropagate(self, x1: float, x2:float, target: float) -> None:\n",
    "        error = (self._output - target)\n",
    "        delta_output = error * sigmoid_derivative(self._outer_sum)\n",
    "        \n",
    "        # Inner layer\n",
    "        for j in range(2):\n",
    "            self._inner_weights[0][j] -= self._learning_rate * delta_output * self._outer_weights[j] * sigmoid_derivative(self._inner_sums[j]) * x1\n",
    "            self._inner_weights[1][j] -= self._learning_rate * delta_output * self._outer_weights[j] * sigmoid_derivative(self._inner_sums[j]) * x2\n",
    "            self._inner_biases[j] -= self._learning_rate * delta_output * self._outer_weights[j] * sigmoid_derivative(self._inner_sums[j])\n",
    "\n",
    "        # Outer layer\n",
    "        for i in range(2):\n",
    "            self._outer_weights[i] -= self._learning_rate * delta_output * self._inner_outputs[i]\n",
    "        self._outer_bias -= self._learning_rate * delta_output\n",
    "\n",
    "    def predict(self, x1: float, x2: float) -> float:\n",
    "        return round(self._feed_forward(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91364b5-8538-4ee5-80f7-fad925feedd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 = 0, x2 = 0\n",
      "Expected: 1\n",
      "Got: 1\n",
      "\n",
      "x1 = 0, x2 = 1\n",
      "Expected: 0\n",
      "Got: 0\n",
      "\n",
      "x1 = 1, x2 = 0\n",
      "Expected: 0\n",
      "Got: 0\n",
      "\n",
      "x1 = 1, x2 = 1\n",
      "Expected: 1\n",
      "Got: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "target = np.array([[1],\n",
    "                     [0],\n",
    "                     [0],\n",
    "                     [1]])\n",
    "\n",
    "network = BinaryTwoOneNetwork(0.5)\n",
    "iterations = 10000\n",
    "for i in range(iterations):\n",
    "    for j in range(4):\n",
    "        network._feed_forward(x[j][0], x[j][1])\n",
    "        network._backpropagate(x[j][0], x[j][1], target[j][0])\n",
    "\n",
    "for i in range(4):\n",
    "    x1 = x[i][0]\n",
    "    x2 = x[i][1]\n",
    "    xnor = network.predict(x1, x2)\n",
    "    print(f\"x1 = {x1}, x2 = {x2}\")\n",
    "    print(f\"Expected: {target[i][0]}\")\n",
    "    print(f\"Got: {xnor}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4530c363-55dd-4549-8dfe-5080bf178487",
   "metadata": {},
   "source": [
    "---\n",
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41dde726-6ced-46ba-b144-c2ad17461cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden w11 = -4.705923271599746\n",
      "hidden w21 = -4.70891917233781\n",
      "hidden bias1 = 6.9992134963942965\n",
      "hidden w12 = -6.544397381740643\n",
      "hidden w22 = -6.565696896610717\n",
      "hidden bias2 = 2.6719085088955197\n",
      "outer w1 = -9.659639429413177\n",
      "outer w2 = 9.775593015536492\n",
      "outer bias = 4.573062147398275\n"
     ]
    }
   ],
   "source": [
    "print(f\"hidden w11 = {network._inner_weights[0][0]}\")\n",
    "print(f\"hidden w21 = {network._inner_weights[1][0]}\")\n",
    "print(f\"hidden bias1 = {network._inner_biases[0]}\")\n",
    "print(f\"hidden w12 = {network._inner_weights[0][1]}\")\n",
    "print(f\"hidden w22 = {network._inner_weights[1][1]}\")\n",
    "print(f\"hidden bias2 = {network._inner_biases[1]}\")\n",
    "print(f\"outer w1 = {network._outer_weights[0]}\")\n",
    "print(f\"outer w2 = {network._outer_weights[1]}\")\n",
    "print(f\"outer bias = {network._outer_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619a390-daeb-48f7-ae3b-50e58fbaa027",
   "metadata": {},
   "source": [
    "The obtained weights are different from the last assignment, altough the weights for the first neuron match the ratio from the and perceptron, the others don't match to any of the previous calculated weights. The weights are different since the network try to approach the actual output to either 1 or 0 when the output is a result of a contiguos function in the interval 0 and 1. This means that even when the predicetd value rounded is correct, the network will keep on approaching values closer to the exact target output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
